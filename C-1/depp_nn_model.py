# -*- coding: utf-8 -*-
"""C1 wee4 Deep NN for images.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EZyVFZcyulwW2MA4Dmom_4oWVQvKIGFZ
"""

# Commented out IPython magic to ensure Python compatibility.
import time
import numpy as np
import h5py
import matplotlib.pyplot as plt
import scipy
from PIL import Image
from scipy import ndimage
from deep_nn_functions import *

# %matplotlib inline
plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

# %load_ext autoreload
# %autoreload 2

np.random.seed(1)

def load_data():
    train_dataset = h5py.File('/content/train_catvnoncat.h5', "r")
    train_set_x_orig = np.array(train_dataset["train_set_x"][:]) # your train set features
    train_set_y_orig = np.array(train_dataset["train_set_y"][:]) # your train set labels

    test_dataset = h5py.File('/content/test_catvnoncat.h5', "r")
    test_set_x_orig = np.array(test_dataset["test_set_x"][:]) # your test set features
    test_set_y_orig = np.array(test_dataset["test_set_y"][:]) # your test set labels

    classes = np.array(test_dataset["list_classes"][:]) # the list of classes
    
    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))
    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))
    
    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes

train_x_orig, train_y, test_x_orig, test_y, classes = load_data()

# example of picture
index = 25
plt.imshow(train_x_orig[index])

print(train_x_orig.shape)
m_train = train_x_orig.shape[0]
num_px = train_x_orig.shape[1]
m_test = test_x_orig.shape[0]
print(test_x_orig.shape)

# reshape the data such that each training example is in a single col

# when we pass -1 in cols it converts data such that the data fits in the no of rows specified
train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T
test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T
print(train_x_flatten.shape)
print(test_x_flatten.shape)

# standardising the data
train_x = train_x_flatten/255
test_x = test_x_flatten/255

# aloof = train_x_flatten.T.reshape(209,64,64,3)

# plt.imshow(aloof[index])

"""2 Layer neural Network"""

n_x = num_px * num_px * 3
n_h = 7
n_y = 1
layer_dims = (n_x, n_h, n_y)

def two_layer_model(X, Y, layer_dims, learning_rate = 0.0075, num_iterations = 300, print_cost = False):
  np.random.seed(1)
  grads = {}
  costs = []
  m = X.shape[1]
  (n_x, n_h, n_y) = layer_dims

  parameters = initialize_parameters(n_x, n_h, n_y)

  W1 = parameters['W1']
  W2 = parameters['W2']
  b1 = parameters['b1']
  b2 = parameters['b2']

  # loop for gradient descent

  for i in range(0, num_iterations):

    # forward propagation L1 = relu, L2 = sigmoid
    # cache stores A[l-1], W[l], b[l], Z[l]
    A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')
    A2, cache2 = linear_activation_forward(A1, W2, b2, 'sigmoid')

    # compute cost
    cost = compute_cost(A2, Y)

    # initiate backward propagation
    dA2 = -(np.divide(Y, A2) - np.divide(1-Y, 1-A2))

    # backward propagation
    dA1, dW2, db2 = linear_activation_backward(dA2, cache2, 'sigmoid')
    dA0, dW1, db1 = linear_activation_backward(dA1, cache1, 'relu')

    # set grads
    grads['dW1'] = dW1
    grads['dW2'] = dW2
    grads['db1'] = db1
    grads['db2'] = db2

    # Update Parameters
    parameters = update_parameters(parameters, grads, learning_rate)

    # Retrieve parameters
    W1 = parameters['W1']
    b1 = parameters['b1']
    W2 = parameters['W2']
    b2 = parameters['b2']

    # Print the cost every 100 training example
    if print_cost and i % 100 == 0:
      print("Cost after iteration {}: {}".format(i, np.squeeze(cost)))
    if print_cost and i % 100 == 0:
      costs.append(cost)

  # plot the cost
  plt.plot(np.squeeze(costs))
  plt.ylabel('cost')
  plt.xlabel('iterations (per hundreds)')
  plt.title("Learning rate =" + str(learning_rate))
  plt.show()

  return parameters

parameters = two_layer_model(train_x, train_y, layer_dims = layer_dims, num_iterations = 2500, print_cost= True, learning_rate=0.0075)

predictions_train = predict(train_x, train_y, parameters)

predictions_test = predict(test_x, test_y, parameters)
print(predictions_test)

"""L layered Neural Network"""

layer_dims = [num_px*num_px*3, 20, 7, 5, 1]

def L_layer_model(X, Y, layer_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost = False):
  costs = []
  np.random.seed(1)

  # initialize the parameters
  parameters = initialize_parameters_deep(layer_dims)

  # gradient descent
  for i in range(0, num_iterations):

    # forward propagation
    AL, caches = L_model_forward(X, parameters)

    # compute cost
    cost = compute_cost(AL, Y)

    # backward propagation
    grads = L_model_backward(AL, Y, caches)

    # update parameters
    parameters = update_parameters(parameters, grads, learning_rate)

    # Print the cost every 100 training example
    if print_cost and i % 100 == 0:
      print ("Cost after iteration %i: %f" %(i, cost))
    if print_cost and i % 100 == 0:
      costs.append(cost)

  # plot the cost
  plt.plot(np.squeeze(costs))
  plt.ylabel('cost')
  plt.xlabel('iterations (per hundreds)')
  plt.title("Learning rate =" + str(learning_rate))
  plt.show()

  return parameters

parameters = L_layer_model(train_x, train_y, layer_dims, num_iterations=2500, print_cost=True, learning_rate=0.009)

pred_train_L = predict(train_x, train_y, parameters)

pred_test_L = predict(test_x, test_y, parameters)

from PIL import Image

fileImage = Image.open("/content/dog-puppy-on-garden-royalty-free-image-1586966191.jpg").convert("RGB").resize([num_px,num_px],Image.ANTIALIAS)
my_label_y = [0] # the true class of your image (1 -> cat, 0 -> non-cat)

image = np.array(fileImage)
my_image = image.reshape(num_px*num_px*3,1)
my_image = my_image/255.
my_predicted_image = predict(my_image, my_label_y, parameters)

plt.imshow(image)
print ("y = " + str(np.squeeze(my_predicted_image)) + ", your L-layer model predicts a \"" + classes[int(np.squeeze(my_predicted_image)),].decode("utf-8") +  "\" picture.")